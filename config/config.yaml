agent:
  class: Agent
  gamma: 0.995
  critic_learning_rate: 2e-3
  actor_learning_rate: 1e-4
  policy_model: type1
  critic_model: type1
  reward_scaling_factor: 100
  noise_params:
    type: normal
    stddev: 0.1
  target_computation_params:
    type: max_steps

replay_buffer:
  class: PrioritizedReplayBuffer
  size: 10000
  alpha: 0.2
  beta: 0.8

environment:
  env_id: LunarLanderContinuous-v2
  seed: 0
  monitor: False

algorithm:
  class: OffPolicyAlgorithm
  training_steps: 10000
  evaluate_every: 10
  train_every: 128
  batch_size: 128
  make_critic_checkpoint: False
  restore_from_checkpoint: False
  return_viewer: False

hydra:
  run:
    dir: ../experiments/${now:%Y-%m-%d}/${now:%H-%M-%S}_${environment.env_id}_clr${agent.critic_learning_rate}_alr${agent.actor_learning_rate}_g${agent.gamma}_n${agent.noise_stddev}
  sweep:
    dir: ../experiments/${now:%Y-%m-%d}/${now:%H-%M-%S}/
    subdir: job${hydra.job.num}_${environment.env_id}_clr${agent.critic_learning_rate}_alr${agent.actor_learning_rate}_g${agent.gamma}_n${agent.noise_stddev}
